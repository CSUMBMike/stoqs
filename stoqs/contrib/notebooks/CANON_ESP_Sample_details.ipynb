{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze and Plot Details for LRAUV ESP Samples from CANON Campaigns\n",
    "*Query databases directly for detailed information that STOQS api requests don't deliver*\n",
    "\n",
    "Executing this Notebook requires a personal STOQS server.  It can be run from either a Docker installation or from a development Vagrant Virtual Machine. \n",
    "\n",
    "### Docker Instructions\n",
    "Install and start the software as \n",
    "[detailed in the README](https://github.com/stoqs/stoqs#production-deployment-with-docker). (Note that on MacOS you will need to modify settings in `docker-compose.yml` and your `.env` files &mdash; look for comments referencing 'HOST_UID'.)\n",
    "        \n",
    "Then, from your `$STOQS_HOME/docker` directory start the Jupyter Notebook server pointing to MBARI's master STOQS database server. Note: firewall rules limit unprivileged access to such resources.\n",
    "\n",
    "    docker-compose exec \\\n",
    "        -e DATABASE_URL=postgis://everyone:guest@kraken.shore.mbari.org:5432/stoqs \\\n",
    "        stoqs stoqs/manage.py shell_plus --notebook\n",
    "\n",
    "A message is displayed giving a URL for you to use in a browser on your host, e.g.:\n",
    "\n",
    "    http://127.0.0.1:8888/?token=<a_token_generated_upon_server_start>\n",
    "\n",
    "In the browser window opened to this URL navigate to this file (`stoqs/contrib/notebooks/CANON_ESP_Sample_details.ipynb`) and open it. You will then be able to execute the cells and modify the code to suit your needs.\n",
    "\n",
    "---\n",
    "\n",
    "### Vagrant VM Instructions\n",
    "Install and provision your VM as [detailed in the README](https://github.com/stoqs/stoqs#getting-started-with-a-stoqs-development-system).\n",
    "\n",
    "Configure for connection to the campaign databases on MBARI's master STOQS database server. Note: firewall rules limit unprivileged access to such resources.\n",
    "\n",
    "\n",
    "    cd $STOQS_HOME/stoqs\n",
    "    ln -s mbari_campaigns.py campaigns.py\n",
    "    export DATABASE_URL=postgis://everyone:guest@kraken.shore.mbari.org:5432/stoqs\n",
    "    \n",
    "Launch the Jupyter Notebook server on your VM with:\n",
    "\n",
    "    cd $STOQS_HOME/stoqs/contrib/notebooks\n",
    "    ../../manage.py shell_plus --notebook\n",
    "\n",
    "A message is displayed giving a URL for you to use in a browser on your host, e.g.:\n",
    "\n",
    "    http://127.0.0.1:8888/?token=<a_token_generated_upon_server_start>\n",
    "\n",
    "Port 8888 on your VM is mapped to port 8887 on your host, so in a web browser on your host open the URL (using the `<a_token_generated_upon_server_start>` printed after the Jupyter Notebook server is started):\n",
    "\n",
    "    http://127.0.0.1:8887/?token=<a_token_generated_upon_server_start>\n",
    "\n",
    "Navigate to this file (stoqs/contrib/notebooks/CANON_ESP_Sample_details.ipynb) and open it. You will then be able to execute the cells and modify the code to suit your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\"\n",
    "\n",
    "# Define CANON Campaigns for which there are LRAUV ESP Samples\n",
    "dbs = {'CN17S': 'stoqs_canon_april2017',\n",
    "       'CN18S': 'stoqs_canon_may2018',\n",
    "       'CN18F': 'stoqs_canon_september2018',\n",
    "       'CN19S': 'stoqs_canon_may2019',\n",
    "       'CN19F': 'stoqs_canon_fall2019',\n",
    "       'CN20S': 'stoqs_canon_july2020',\n",
    "       'CN20F': 'stoqs_canon_october2020',\n",
    "      }\n",
    "\n",
    "# Save the ESP Sample identifers (Activity name) in each Campaign\n",
    "db_samples = defaultdict(list)\n",
    "for cid, db in dbs.items():\n",
    "    print(f\"{cid}: {db}\")\n",
    "    for sample in (Sample.objects.using(db)\n",
    "                   .filter(instantpoint__activity__platform__name__contains='ESP')\n",
    "                   .values_list('instantpoint__activity__name', flat=True)):\n",
    "        ##print(f\"\\t{sample}\")\n",
    "        db_samples[cid].append(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cid, samples in db_samples.items():\n",
    "    print(f\"{cid}\")\n",
    "    for sample in samples:\n",
    "        print(f\"\\t{sample}\")\n",
    "        sample_locations = (Measurement.objects.using(dbs[cid])\n",
    "                            .filter(instantpoint__activity__name=sample))\n",
    "        for location in (Measurement.objects.using(dbs[cid])\n",
    "                         .filter(instantpoint__activity__name=sample)):\n",
    "            lon, lat, depth = (location.geom.x, location.geom.y, location.depth)\n",
    "            print(f\"{lon}, {lat}, {depth}\")\n",
    "            \n",
    "        breakpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts = Activity.objects.using(db).filter(instantpoint__measurement__in=near_ts_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts.values_list('platform__name', flat=True).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pctds = acts.filter(platform__name='WesternFlyer_PCTD').order_by('startdate').distinct()\n",
    "esps = acts.filter(platform__name='makai_ESP_Archive').order_by('startdate').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pctds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pylab as plt\n",
    "plt.scatter([pctd.mappoint.x for pctd in pctds],\n",
    "            [pctd.mappoint.y for pctd in pctds], c='b')\n",
    "plt.scatter([esp.mappoint.x for esp in esps],\n",
    "            [esp.mappoint.y for esp in esps], c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pylab\n",
    "from numpy import arange\n",
    "import operator\n",
    "\n",
    "def plot_platforms(ax):\n",
    "    plat_labels = []\n",
    "\n",
    "    # Plot in order by platformtype name and platform name\n",
    "    for ypos, plat in enumerate(\n",
    "                        sorted(plat_start_ends.keys(),\n",
    "                               key=operator.attrgetter('platformtype.name', 'name'))):\n",
    "        plat_labels.append(f'{plat.name} ({plat.platformtype.name})')    \n",
    "        for bdate, edate in plat_start_ends[plat]:\n",
    "            dd = edate - bdate\n",
    "            if dd < 1:\n",
    "                dd = 1\n",
    "            ax.barh(ypos+0.5, dd, left=bdate, height=0.8, \n",
    "                    align='center', color='#' + plat.color, alpha=1.0) \n",
    "\n",
    "    ax.set_title(Campaign.objects.using(db).get(id=1).description)\n",
    "    ax.set_ylim(-0.5, len(plat_labels) + 0.5)\n",
    "    ax.set_yticks(arange(len(plat_labels)) + 0.5)\n",
    "    ax.set_yticklabels(plat_labels)\n",
    "\n",
    "    ax.grid(True)\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%B %Y'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    plt.gca().xaxis.set_minor_locator(mdates.DayLocator())\n",
    "    plt.gcf().autofmt_xdate()\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (15, 9)\n",
    "fig, ax = plt.subplots()\n",
    "plot_platforms(ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appear to be 10 events measured by the Benthic Event Detectors. Let's find the start times for these events and use _k_-means clustering to group the BEDs event data start times into 10 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "bed_starts = np.array(Activity.objects.using(db)\n",
    "                              .filter(platform__name__contains='BED')\n",
    "                              .values_list('startdate', flat=True)\n",
    "                              .order_by('startdate'), dtype=np.datetime64)\n",
    "km = KMeans(n_clusters=10).fit(bed_starts.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick the earliest event start time and construct start and end times that we'll use to instruct the STOQS loader that these are the times when we want to load ADCP data from all the moorings into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = {}\n",
    "for bed_start in bed_starts:\n",
    "    label = km.predict(bed_start.reshape(-1, 1))[0]\n",
    "    if label not in events.keys():\n",
    "        events[label] = bed_start\n",
    "    # Print the clusters of start times and tune n_clusters above to get the optimal set\n",
    "    ##print(bed_start, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print `Event()` instances of begining and end times for use in [loadCCE_2015.py](https://github.com/stoqs/stoqs/blob/3a596e6791104054c676a0ba84e81ec02b7ca06b/stoqs/loaders/CCE/loadCCE_2015.py#L23-L32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "event_start_ends = defaultdict(list)\n",
    "def print_Events(events, before, after, type):\n",
    "    for start in events.values():\n",
    "        beg_dt = repr(start.astype(datetime) - before).replace('datetime.', '')\n",
    "        end_dt = repr(start.astype(datetime) + after).replace('datetime.', '')\n",
    "        event_start_ends[type].append((mdates.date2num(start.astype(datetime) - before),\n",
    "                                       mdates.date2num(start.astype(datetime) + after)))\n",
    "        print(f\"        Event({beg_dt}, {end_dt}),\")\n",
    "\n",
    "# Low-resolution region: 1 day before to 2 days after the start of each event\n",
    "before = timedelta(days=1)\n",
    "after = timedelta(days=2)\n",
    "print(\"lores_event_times = [\")\n",
    "print_Events(events, before, after, 'lores')\n",
    "print(\"                    ]\")\n",
    "\n",
    "# High-resolution region: 4 hours before to 14 hours after the start of each event\n",
    "before = timedelta(hours=4)\n",
    "after = timedelta(hours=14)\n",
    "print(\"hires_event_times = [\")\n",
    "print_Events(events, before, after, 'hires')\n",
    "print(\"                    ]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot timeline again, but this time with events as shaded regions across all the Platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_events(ax):\n",
    "    for type in ('lores', 'hires'):\n",
    "        for bdate, edate in event_start_ends[type]:\n",
    "            dd = edate - bdate\n",
    "            if dd < 1:\n",
    "                dd = 1\n",
    "            # Plot discovered events as gray lines across all platforms\n",
    "            ax.barh(0, dd, left=bdate, height=32, \n",
    "                    align='center', color='#000000', alpha=0.1) \n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (15, 9)\n",
    "fig, ax2 = plt.subplots()\n",
    "plot_platforms(ax2)\n",
    "plot_events(ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
